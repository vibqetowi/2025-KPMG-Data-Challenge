import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime

# File paths
BUDGET_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Budget.csv')
STAFFING_2024_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Staffing___2024.csv')
STAFFING_2025_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Staffing___2025.csv')
TIME_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_TIME.csv')
DICTIONARY_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Dictionnaire___Dictionary_.csv')

# Output directory
OUTPUT_DIR = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/transformed')

class DataTransformer:
    def __init__(self):
        """
        Initialize the data transformer with file paths and schema definitions
        based on the SQL DDL structure.
        """
        # Ensure output directory exists
        OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
        
        # Define table schemas based on DDL with proper types
        self.schemas = {
            'employees': {
                'columns': ['personnel_no', 'employee_name', 'staff_level', 'is_external', 'employment_basis'],
                'dtypes': {'personnel_no': 'int', 'employee_name': 'str', 'staff_level': 'str', 
                          'is_external': 'bool', 'employment_basis': 'float'}
            },
            'clients': {
                'columns': ['client_no', 'client_name'],
                'dtypes': {'client_no': 'int', 'client_name': 'str'}
            },
            'engagements': {
                'columns': ['eng_no', 'eng_description', 'client_no'],
                'dtypes': {'eng_no': 'int64', 'eng_description': 'str', 'client_no': 'int'}
            },
            'phases': {
                'columns': ['eng_no', 'eng_phase', 'phase_description', 'budget'],
                'dtypes': {'eng_no': 'int64', 'eng_phase': 'int', 'phase_description': 'str', 'budget': 'float'}
            },
            'staffing': {
                'columns': ['id', 'personnel_no', 'eng_no', 'eng_phase', 'week_start_date', 'planned_hours'],
                'dtypes': {'id': 'int', 'personnel_no': 'int', 'eng_no': 'int64', 
                          'eng_phase': 'int', 'week_start_date': 'datetime64[ns]', 'planned_hours': 'float'}
            },
            'timesheets': {
                'columns': ['id', 'personnel_no', 'eng_no', 'eng_phase', 'work_date', 'hours', 
                           'time_entry_date', 'posting_date', 'charge_out_rate', 'std_price', 'adm_surcharge'],
                'dtypes': {'id': 'int', 'personnel_no': 'int', 'eng_no': 'int64', 'eng_phase': 'int',
                          'work_date': 'datetime64[ns]', 'hours': 'float', 'time_entry_date': 'datetime64[ns]',
                          'posting_date': 'datetime64[ns]', 'charge_out_rate': 'float', 
                          'std_price': 'float', 'adm_surcharge': 'float'}
            },
            'dictionary': {
                'columns': ['key', 'description'],
                'dtypes': {'key': 'str', 'description': 'str'}
            },
            'vacations': {
                'columns': ['personnel_no', 'start_date', 'end_date'],
                'dtypes': {'personnel_no': 'int', 'start_date': 'datetime64[ns]', 'end_date': 'datetime64[ns]'}
            }
        }
        
        # Define primary keys for each table as per DDL
        self.primary_keys = {
            'employees': ['personnel_no'],
            'clients': ['client_no'],
            'engagements': ['eng_no'],
            'phases': ['eng_no', 'eng_phase'],  # Composite primary key
            'staffing': ['id'],  # Auto-generated by DB
            'timesheets': ['id'],  # Auto-generated by DB
            'dictionary': ['key'],
            'vacations': ['personnel_no', 'start_date']  # Composite primary key
        }
        
        # Define foreign key relationships as per DDL
        self.foreign_keys = {
            'engagements': [('client_no', 'clients', 'client_no')],
            'phases': [('eng_no', 'engagements', 'eng_no')],
            'staffing': [
                ('personnel_no', 'employees', 'personnel_no'),
                (('eng_no', 'eng_phase'), 'phases', ('eng_no', 'eng_phase'))
            ],
            'timesheets': [
                ('personnel_no', 'employees', 'personnel_no'),
                (('eng_no', 'eng_phase'), 'phases', ('eng_no', 'eng_phase'))
            ],
            'vacations': [('personnel_no', 'employees', 'personnel_no')]
        }
        
        # Columns generated by the database (IDENTITY in SQL Server)
        self.db_generated_columns = {
            'staffing': ['id'],
            'timesheets': ['id']
        }
        
        # Initialize empty DataFrames for each table
        self.dfs = {}
        for table, schema in self.schemas.items():
            self.dfs[table] = pd.DataFrame(columns=schema['columns'])
    
    def _clean_dataframe(self, df, table_name):
        """
        Clean and validate a dataframe according to its schema.
        """
        schema = self.schemas[table_name]
        columns = schema['columns']
        dtypes = schema['dtypes']
        
        # Select only columns defined in the schema
        df = df.reindex(columns=columns)
        
        # Replace empty strings with None
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].replace('', None)
        
        # Set NULL for database-generated columns
        for col in self.db_generated_columns.get(table_name, []):
            df[col] = None
        
        # Convert data types based on schema
        for col, dtype in dtypes.items():
            if col in df.columns:
                try:
                    if dtype in ('datetime64[ns]', 'datetime'):
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    elif dtype == 'int' or dtype == 'int64':
                        # Convert to float first to handle NaNs, then to Int64 (nullable integer)
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        df[col] = df[col].astype('Int64')
                    elif dtype == 'float':
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    elif dtype == 'bool':
                        df[col] = df[col].astype(bool)
                except Exception as e:
                    print(f"Error converting {col} to {dtype} in {table_name}: {e}")
        
        return df
    
    def process_budget_data(self):
        """
        Process budget data to populate engagements and phases tables.
        Ensuring composite primary keys are preserved.
        """
        if not os.path.exists(BUDGET_CSV):
            print(f"Warning: Budget CSV file not found at {BUDGET_CSV}")
            return
        
        try:
            print(f"Processing budget file: {BUDGET_CSV}")
            df_budget = pd.read_csv(BUDGET_CSV, encoding='utf-8-sig')
            
            # Process engagements
            engagements = []
            phases = []
            
            for _, row in df_budget.iterrows():
                # Extract engagement data
                eng_no = row['Code projet']
                eng_description = row['Nom de projet']
                
                # Add to engagements
                engagements.append({
                    'eng_no': eng_no,
                    'eng_description': eng_description,
                    'client_no': None  # Will be updated from staffing data
                })
                
                # Extract phase data - make sure to include both parts of the composite primary key
                phases.append({
                    'eng_no': eng_no,  # First part of composite PK
                    'eng_phase': int(row['Code phase']),  # Second part of composite PK
                    'phase_description': row['Phase Projet'],
                    'budget': float(row['Budget']) if pd.notna(row['Budget']) else None
                })
            
            # Convert to DataFrames
            if engagements:
                df_engagements = pd.DataFrame(engagements)
                df_engagements = df_engagements.drop_duplicates(subset=['eng_no'])
                self.dfs['engagements'] = pd.concat([self.dfs['engagements'], df_engagements])
                self.dfs['engagements'] = self.dfs['engagements'].drop_duplicates(subset=['eng_no'], keep='last')
                
            if phases:
                df_phases = pd.DataFrame(phases)
                # Print the first 5 rows to verify composite primary key is present
                print("\nPhases composite primary key verification (raw data):")
                print(df_phases[['eng_no', 'eng_phase']].head())
                
                # Ensure both parts of the composite key are present and not null
                df_phases = df_phases[df_phases['eng_no'].notna() & df_phases['eng_phase'].notna()]
                
                df_phases = df_phases.drop_duplicates(subset=['eng_no', 'eng_phase'])
                self.dfs['phases'] = pd.concat([self.dfs['phases'], df_phases])
                self.dfs['phases'] = self.dfs['phases'].drop_duplicates(subset=['eng_no', 'eng_phase'], keep='last')
            
            print(f"Processed budget data: {len(engagements)} engagements, {len(phases)} phases")
            
        except Exception as e:
            print(f"Error processing budget data: {str(e)}")
    
    def process_staffing_data(self):
        """
        Process staffing data to populate employees, clients, and staffing tables.
        Also updates engagement client relationships.
        """
        staffing_files = [STAFFING_2024_CSV, STAFFING_2025_CSV]
        employees = []
        clients = []
        staffing_records = []
        engagement_client_map = {}
        
        for staffing_file in staffing_files:
            if not os.path.exists(staffing_file):
                print(f"Warning: Staffing file not found at {staffing_file}")
                continue
            
            try:
                print(f"Processing staffing file: {staffing_file}")
                df_staff = pd.read_csv(staffing_file, encoding='utf-8-sig')
                
                # Extract employees
                for _, row in df_staff.iterrows():
                    personnel_no = row['Personnel No.']
                    
                    # Extract employee data
                    employees.append({
                        'personnel_no': personnel_no,
                        'employee_name': row['Employee Name'],
                        'staff_level': row['Staff Level'],
                        'is_external': False,  # Default to internal
                        'employment_basis': 40.0  # Set 40 hours weekly basis for everyone
                    })
                    
                    # Extract client data
                    client_no = row['Client No.']
                    clients.append({
                        'client_no': client_no,
                        'client_name': row['Client Name']
                    })
                    
                    # Map engagement to client
                    eng_no = row['Eng. No.']
                    engagement_client_map[eng_no] = client_no
                    
                    # Process staffing data - columns after 'Staff Level' are week dates
                    date_columns = [col for col in df_staff.columns[9:] if 'Unnamed' not in col]
                    
                    for date_col in date_columns:
                        hours = row[date_col]
                        if pd.notna(hours) and hours > 0:
                            # Convert string date to datetime
                            try:
                                # Handle different date formats
                                week_date = pd.to_datetime(date_col.replace(' 00:00:00', ''))
                                
                                staffing_records.append({
                                    'id': None,  # DB will generate
                                    'personnel_no': personnel_no,
                                    'eng_no': eng_no,
                                    'eng_phase': row['Eng. Phase'],
                                    'week_start_date': week_date,
                                    'planned_hours': float(hours)
                                })
                            except:
                                print(f"Warning: Could not parse date {date_col}")
                
            except Exception as e:
                print(f"Error processing staffing file {staffing_file}: {str(e)}")
        
        # Convert to DataFrames and deduplicate
        if employees:
            df_employees = pd.DataFrame(employees).drop_duplicates(subset=['personnel_no'])
            self.dfs['employees'] = pd.concat([self.dfs['employees'], df_employees])
            self.dfs['employees'] = self.dfs['employees'].drop_duplicates(subset=['personnel_no'], keep='last')
        
        if clients:
            df_clients = pd.DataFrame(clients).drop_duplicates(subset=['client_no'])
            self.dfs['clients'] = pd.concat([self.dfs['clients'], df_clients])
            self.dfs['clients'] = self.dfs['clients'].drop_duplicates(subset=['client_no'], keep='last')
        
        if staffing_records:
            df_staffing = pd.DataFrame(staffing_records)
            self.dfs['staffing'] = pd.concat([self.dfs['staffing'], df_staffing])
        
        # Update engagement-client relationships
        if engagement_client_map and not self.dfs['engagements'].empty:
            for idx, row in self.dfs['engagements'].iterrows():
                eng_no = row['eng_no']
                if eng_no in engagement_client_map:
                    self.dfs['engagements'].at[idx, 'client_no'] = engagement_client_map[eng_no]
        
        print(f"Processed staffing data: {len(self.dfs['employees'])} employees, {len(self.dfs['clients'])} clients, "
              f"{len(self.dfs['staffing'])} staffing records")
    
    def process_timesheet_data(self):
        """
        Process timesheet data to populate the timesheets table.
        """
        if not os.path.exists(TIME_CSV):
            print(f"Warning: Timesheet file not found at {TIME_CSV}")
            return
        
        try:
            print(f"Processing timesheet file: {TIME_CSV}")
            
            # Try different encodings
            encodings = ['utf-8-sig', 'latin1', 'cp1252']
            df_time = None
            
            for encoding in encodings:
                try:
                    df_time = pd.read_csv(TIME_CSV, encoding=encoding)
                    break
                except:
                    continue
            
            if df_time is None or df_time.empty:
                print(f"Warning: Could not read timesheet file or file is empty")
                return
            
            # Map column names
            column_mapping = {
                'Personnel No.': 'personnel_no',
                'Work Date': 'work_date',
                'Hours': 'hours',
                'Eng. No.': 'eng_no',
                'Eng. Phase': 'eng_phase',
                'Time Entry Date': 'time_entry_date',
                'Posting Date': 'posting_date',
                'Charge-Out Rate': 'charge_out_rate',
                'Std. Price': 'std_price',
                'Adm. Surcharge': 'adm_surcharge'
            }
            
            # Check for required columns
            missing_columns = [col for col in column_mapping.keys() if col not in df_time.columns]
            if missing_columns:
                print(f"Warning: Missing columns in timesheet file: {missing_columns}")
                print(f"Available columns: {df_time.columns.tolist()}")
                return
            
            # Rename columns and set ID to None
            df_time = df_time.rename(columns=column_mapping)
            df_time['id'] = None
            
            # Convert date columns
            date_columns = ['work_date', 'time_entry_date', 'posting_date']
            for col in date_columns:
                if col in df_time.columns:
                    df_time[col] = pd.to_datetime(df_time[col], errors='coerce')
            
            # Convert numeric columns
            numeric_columns = ['hours', 'charge_out_rate', 'std_price', 'adm_surcharge']
            for col in numeric_columns:
                if col in df_time.columns:
                    df_time[col] = pd.to_numeric(df_time[col], errors='coerce')
            
            # Select only the columns defined in our schema
            schema_columns = self.schemas['timesheets']['columns']
            df_time = df_time.reindex(columns=schema_columns)
            
            # Append to the timesheets DataFrame
            self.dfs['timesheets'] = pd.concat([self.dfs['timesheets'], df_time])
            
            print(f"Processed {len(df_time)} timesheet records")
            
        except Exception as e:
            print(f"Error processing timesheet data: {str(e)}")
    
    def validate_foreign_keys(self):
        """
        Validate foreign key relationships between tables.
        """
        for table, fk_list in self.foreign_keys.items():
            if table not in self.dfs or self.dfs[table].empty:
                continue
                
            for fk in fk_list:
                if isinstance(fk[0], tuple):
                    # Composite foreign key
                    fk_cols = fk[0]
                    ref_table = fk[1]
                    ref_cols = fk[2]
                    
                    if ref_table not in self.dfs or self.dfs[ref_table].empty:
                        continue
                        
                    # Create sets of reference keys
                    ref_keys = set()
                    for _, row in self.dfs[ref_table].iterrows():
                        key_tuple = tuple(row[col] for col in ref_cols)
                        ref_keys.add(key_tuple)
                    
                    # Check each row in the table
                    valid_rows = []
                    for idx, row in self.dfs[table].iterrows():
                        fk_tuple = tuple(row[col] for col in fk_cols)
                        
                        # Allow if any FK column is NULL
                        if any(pd.isna(val) for val in fk_tuple):
                            valid_rows.append(idx)
                        # Otherwise check if the FK exists in the reference table
                        elif fk_tuple in ref_keys:
                            valid_rows.append(idx)
                    
                    # Keep only valid rows
                    self.dfs[table] = self.dfs[table].loc[valid_rows]
                
                else:
                    # Simple foreign key
                    fk_col = fk[0]
                    ref_table = fk[1]
                    ref_col = fk[2]
                    
                    if ref_table not in self.dfs or self.dfs[ref_table].empty:
                        continue
                        
                    # Get valid reference keys
                    ref_keys = set(self.dfs[ref_table][ref_col].dropna().unique())
                    
                    # Filter rows with valid foreign keys or NULL
                    self.dfs[table] = self.dfs[table][
                        self.dfs[table][fk_col].isna() | self.dfs[table][fk_col].isin(ref_keys)
                    ]
        
        # Report counts after validation
        for table, df in self.dfs.items():
            print(f"After FK validation: {table} has {len(df)} records")
    
    def save_transformed_data(self):
        """
        Save all DataFrames to CSV files in the output directory,
        explicitly ensuring composite primary keys are included.
        """
        for table, df in self.dfs.items():
            if df.empty:
                print(f"Skipping empty table: {table}")
                continue
            
            # Clean the DataFrame according to its schema
            df = self._clean_dataframe(df, table)
            
            # Make sure composite primary keys are present
            pk_columns = self.primary_keys.get(table, [])
            if len(pk_columns) > 1:  # Composite primary key
                print(f"\nProcessing table {table} with composite primary key {pk_columns}")
                
                # Filter to keep only rows where ALL primary key components are non-null
                valid_mask = df[pk_columns].notna().all(axis=1)
                before_filter = len(df)
                df = df[valid_mask]
                after_filter = len(df)
                
                if before_filter != after_filter:
                    print(f"Filtered out {before_filter - after_filter} rows with NULL in composite primary key columns")
                
                # Print the first 5 rows to verify composite primary key columns
                print(f"\nComposite primary key columns in {table}:")
                print(df[pk_columns].head(5))
            
            # Set auto-generated columns to NULL
            if table in self.db_generated_columns:
                for col in self.db_generated_columns[table]:
                    df[col] = None
            
            # Save to CSV with NULL representation for NaN values
            output_file = OUTPUT_DIR / f"{table}.csv"
            df.to_csv(output_file, index=False, na_rep='NULL')
            
            # Verify the saved CSV actually contains the composite primary key columns
            if len(pk_columns) > 1:
                # Read back the saved CSV to verify
                try:
                    verification_df = pd.read_csv(output_file)
                    print(f"\nVerifying composite primary key in saved CSV for {table}:")
                    print(verification_df[pk_columns].head(5))
                    
                    # Verify that all primary key columns exist
                    if all(col in verification_df.columns for col in pk_columns):
                        print(f"✓ All composite primary key columns {pk_columns} are present in the CSV")
                    else:
                        print(f"❌ ERROR: Not all composite primary key columns {pk_columns} are in the CSV!")
                        missing = [col for col in pk_columns if col not in verification_df.columns]
                        print(f"   Missing columns: {missing}")
                except Exception as e:
                    print(f"Error verifying composite primary key in saved CSV: {e}")
            
            # Report what was saved
            if len(pk_columns) > 1:
                print(f"Saved {len(df)} rows to {output_file} with composite primary key: {pk_columns}")
            else:
                print(f"Saved {len(df)} rows to {output_file}")

    def generate_placeholder_vacation_data(self):
        """
        Generate 5 sample vacation periods in 2025 for real employees.
        """
        if 'employees' not in self.dfs or self.dfs['employees'].empty:
            print("No employee data available to generate vacation placeholders")
            return
        
        # Define 5 vacation periods for 2025
        vacation_periods = [
            # Spring break
            ('2025-03-10', '2025-03-17'),
            # Summer vacations
            ('2025-07-01', '2025-07-15'),
            ('2025-08-03', '2025-08-17'),
            # Fall break
            ('2025-10-20', '2025-10-27'),
            # Winter holidays
            ('2025-12-22', '2025-12-31')
        ]
        
        vacations = []
        # Get all personnel numbers from employees DataFrame
        personnel_nos = self.dfs['employees']['personnel_no'].dropna().unique().tolist()
        
        if personnel_nos:
            print(f"Generating vacations for {len(personnel_nos)} employees")
            
            # Distribute vacations among employees
            # Each employee gets at least one vacation period
            for i, personnel_no in enumerate(personnel_nos):
                # Determine which vacation period to assign (cycling through the 5 periods)
                vac_index = i % len(vacation_periods)
                start_date, end_date = vacation_periods[vac_index]
                
                vacations.append({
                    'personnel_no': personnel_no,
                    'start_date': pd.Timestamp(start_date),
                    'end_date': pd.Timestamp(end_date)
                })
                
                # Add a second vacation for some employees
                if i % 3 == 0:  # Every third employee gets a second vacation
                    second_vac_index = (vac_index + 2) % len(vacation_periods)
                    second_start, second_end = vacation_periods[second_vac_index]
                    
                    vacations.append({
                        'personnel_no': personnel_no,
                        'start_date': pd.Timestamp(second_start),
                        'end_date': pd.Timestamp(second_end)
                    })
        
        if vacations:
            df_vacations = pd.DataFrame(vacations)
            df_vacations = df_vacations.drop_duplicates(subset=['personnel_no', 'start_date'])
            self.dfs['vacations'] = pd.concat([self.dfs.get('vacations', pd.DataFrame()), df_vacations])
            self.dfs['vacations'] = self.dfs['vacations'].drop_duplicates(subset=['personnel_no', 'start_date'], keep='last')
            
            print(f"Generated {len(vacations)} vacation records for {len(personnel_nos)} employees")
            # Preview the first few vacation records
            print("Sample vacation entries:")
            print(self.dfs['vacations'].head())

    def transform_all_data(self):
        """
        Execute the complete data transformation pipeline.
        """
        print(f"Starting data transformation at {datetime.now()}")
        
        # Process each data source
        self.process_budget_data()
        self.process_staffing_data()
        self.process_timesheet_data()
        # Skipping dictionary processing as requested
        print("Skipping dictionary data processing as requested")
        
        # Generate 40-hour employment basis and vacation data
        print("Setting 40-hour employment basis for all employees")
        self.generate_placeholder_vacation_data()
        
        # Validate foreign key relationships
        self.validate_foreign_keys()
        
        # Save transformed data
        self.save_transformed_data()
        
        print(f"Data transformation completed at {datetime.now()}")

def main():
    """
    Main function to run the data transformer.
    """
    try:
        transformer = DataTransformer()
        transformer.transform_all_data()
    except Exception as e:
        print(f"Error in data transformation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
