import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime

# File paths
BUDGET_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Budget.csv')
STAFFING_2024_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Staffing___2024.csv')
STAFFING_2025_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Staffing___2025.csv')
TIME_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_TIME.csv')
DICTIONARY_CSV = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/KPMG Case Data_Dictionnaire___Dictionary_.csv')

# Output directory
OUTPUT_DIR = Path('/Users/notAdmin/Dev/2025 KPMG Data Challenge/csv-dump/transformed')

class DataTransformer:
    def __init__(self):
        """
        Initialize the data transformer with file paths and schema definitions
        based on the SQL DDL structure.
        """
        # Ensure output directory exists
        OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
        
        # Define table schemas based on DDL with proper types
        self.schemas = {
            'practices': {
                'columns': ['practice_id', 'practice_name', 'description'],
                'dtypes': {'practice_id': 'int', 'practice_name': 'str', 'description': 'str'}
            },
            'employees': {
                'columns': ['personnel_no', 'employee_name', 'staff_level', 'is_external', 'employment_basis', 'practice_id'],
                'dtypes': {'personnel_no': 'int', 'employee_name': 'str', 'staff_level': 'str', 
                          'is_external': 'bool', 'employment_basis': 'float', 'practice_id': 'int'}
            },
            'clients': {
                'columns': ['client_no', 'client_name'],
                'dtypes': {'client_no': 'int', 'client_name': 'str'}
            },
            'engagements': {
                'columns': ['eng_no', 'eng_description', 'client_no', 'start_date', 'end_date', 
                           'actual_end_date', 'primary_practice_id'],
                'dtypes': {'eng_no': 'int64', 'eng_description': 'str', 'client_no': 'int',
                          'start_date': 'datetime64[ns]', 'end_date': 'datetime64[ns]', 
                          'actual_end_date': 'datetime64[ns]', 'primary_practice_id': 'int'}
            },
            'phases': {
                'columns': ['eng_no', 'eng_phase', 'phase_description', 'budget', 
                           'start_date', 'end_date', 'actual_end_date'],
                'dtypes': {'eng_no': 'int64', 'eng_phase': 'int', 'phase_description': 'str', 'budget': 'float',
                          'start_date': 'datetime64[ns]', 'end_date': 'datetime64[ns]', 
                          'actual_end_date': 'datetime64[ns]'}
            },
            'staffing': {
                'columns': ['id', 'personnel_no', 'eng_no', 'eng_phase', 'week_start_date', 'planned_hours'],
                'dtypes': {'id': 'int', 'personnel_no': 'int', 'eng_no': 'int64', 
                          'eng_phase': 'int', 'week_start_date': 'datetime64[ns]', 'planned_hours': 'float'}
            },
            'timesheets': {
                'columns': ['id', 'personnel_no', 'eng_no', 'eng_phase', 'work_date', 'hours', 
                           'time_entry_date', 'posting_date', 'charge_out_rate', 'std_price', 'adm_surcharge'],
                'dtypes': {'id': 'int', 'personnel_no': 'int', 'eng_no': 'int64', 'eng_phase': 'int',
                          'work_date': 'datetime64[ns]', 'hours': 'float', 'time_entry_date': 'datetime64[ns]',
                          'posting_date': 'datetime64[ns]', 'charge_out_rate': 'float', 
                          'std_price': 'float', 'adm_surcharge': 'float'}
            },
            'dictionary': {
                'columns': ['key', 'description'],
                'dtypes': {'key': 'str', 'description': 'str'}
            },
            'vacations': {
                'columns': ['personnel_no', 'start_date', 'end_date'],
                'dtypes': {'personnel_no': 'int', 'start_date': 'datetime64[ns]', 'end_date': 'datetime64[ns]'}
            }
        }
        
        # Define primary keys for each table as per DDL
        self.primary_keys = {
            'practices': ['practice_id'],
            'employees': ['personnel_no'],
            'clients': ['client_no'],
            'engagements': ['eng_no'],
            'phases': ['eng_no', 'eng_phase'],  # Composite primary key
            'staffing': ['id'],  # Auto-generated by DB
            'timesheets': ['id'],  # Auto-generated by DB
            'dictionary': ['key'],
            'vacations': ['personnel_no', 'start_date']  # Composite primary key
        }
        
        # Define foreign key relationships as per DDL
        self.foreign_keys = {
            'employees': [('practice_id', 'practices', 'practice_id')],
            'engagements': [
                ('client_no', 'clients', 'client_no'),
                ('primary_practice_id', 'practices', 'practice_id')
            ],
            'phases': [('eng_no', 'engagements', 'eng_no')],
            'staffing': [
                ('personnel_no', 'employees', 'personnel_no'),
                (('eng_no', 'eng_phase'), 'phases', ('eng_no', 'eng_phase'))
            ],
            'timesheets': [
                ('personnel_no', 'employees', 'personnel_no'),
                (('eng_no', 'eng_phase'), 'phases', ('eng_no', 'eng_phase'))
            ],
            'vacations': [('personnel_no', 'employees', 'personnel_no')]
        }
        
        # Columns generated by the database (IDENTITY in SQL Server)
        self.db_generated_columns = {
            'staffing': ['id'],
            'timesheets': ['id']
        }
        
        # Initialize empty DataFrames for each table
        self.dfs = {}
        for table, schema in self.schemas.items():
            self.dfs[table] = pd.DataFrame(columns=schema['columns'])
        
        # Create default practice
        self._create_default_practice()
    
    def _create_default_practice(self):
        """
        Create KPMG-specific practices, with SAP as practice ID 1
        """
        practices = [
            {
                'practice_id': 1,
                'practice_name': 'SAP',
                'description': 'SAP Implementation and Advisory Services'
            },
            {
                'practice_id': 2,
                'practice_name': 'Audit & Assurance',
                'description': 'Financial Statement Audits and Assurance Services'
            },
            {
                'practice_id': 3,
                'practice_name': 'Cloud Services',
                'description': 'AWS, Azure, and GCP Implementation Advisory'
            },
            {
                'practice_id': 4, 
                'practice_name': 'Management Consulting',
                'description': 'Strategy and Operations Consulting'
            },
            {
                'practice_id': 5,
                'practice_name': 'Deal Advisory',
                'description': 'Mergers & Acquisitions and Transaction Services'
            },
            {
                'practice_id': 6,
                'practice_name': 'Risk Consulting',
                'description': 'Risk Management, Compliance, and Governance'
            },
            {
                'practice_id': 7,
                'practice_name': 'Technology Consulting',
                'description': 'Digital Transformation and Enterprise Technologies'
            },
            {
                'practice_id': 8,
                'practice_name': 'ESG Advisory',
                'description': 'Environmental, Social, and Governance Services'
            }
        ]
        
        self.dfs['practices'] = pd.DataFrame(practices)
        print(f"Created {len(practices)} KPMG practice records (SAP is default with ID 1)")
        print("Available KPMG practices:")
        for _, practice in enumerate(practices):
            print(f"  - {practice['practice_id']}: {practice['practice_name']}")
    
    def _clean_dataframe(self, df, table_name):
        """
        Clean and validate a dataframe according to its schema.
        """
        schema = self.schemas[table_name]
        columns = schema['columns']
        dtypes = schema['dtypes']
        
        # Select only columns defined in the schema
        df = df.reindex(columns=columns)
        
        # Replace empty strings with None
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].replace('', None)
        
        # Set NULL for database-generated columns
        for col in self.db_generated_columns.get(table_name, []):
            df[col] = None
        
        # Convert data types based on schema
        for col, dtype in dtypes.items():
            if col in df.columns:
                try:
                    if dtype in ('datetime64[ns]', 'datetime'):
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    elif dtype == 'int' or dtype == 'int64':
                        # Convert to float first to handle NaNs, then to Int64 (nullable integer)
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        df[col] = df[col].astype('Int64')
                    elif dtype == 'float':
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    elif dtype == 'bool':
                        df[col] = df[col].astype(bool)
                except Exception as e:
                    print(f"Error converting {col} to {dtype} in {table_name}: {e}")
        
        return df
    
    def _drop_null_columns(self, df, table_name, exclude_columns=None):
        """
        Drop columns that are entirely NULL, except those in exclude_columns
        Returns the DataFrame with NULL columns removed
        """
        if exclude_columns is None:
            exclude_columns = []
        
        # Ensure exclude_columns is a list
        if not isinstance(exclude_columns, list):
            exclude_columns = [exclude_columns]
        
        # Add primary key columns to exclude_columns
        pk_columns = self.primary_keys.get(table_name, [])
        if not isinstance(pk_columns, list):
            pk_columns = [pk_columns]
        exclude_columns.extend(pk_columns)
        
        # Find columns that are all NULL and not in exclude_columns
        null_columns = []
        for col in df.columns:
            if col not in exclude_columns and df[col].isna().all():
                null_columns.append(col)
        
        if null_columns:
            print(f"Dropping all-NULL columns from {table_name} before generating values: {null_columns}")
            df = df.drop(columns=null_columns)
        
        return df

    def process_budget_data(self):
        """
        Process budget data to populate engagements and phases tables.
        Ensuring composite primary keys are preserved.
        """
        if not os.path.exists(BUDGET_CSV):
            print(f"Warning: Budget CSV file not found at {BUDGET_CSV}")
            return
        
        try:
            print(f"Processing budget file: {BUDGET_CSV}")
            df_budget = pd.read_csv(BUDGET_CSV, encoding='utf-8-sig')
            
            # Process engagements - only include non-NULL fields and required keys
            engagements = []
            phases = []
            
            for _, row in df_budget.iterrows():
                # Extract engagement data
                eng_no = row['Code projet']
                eng_description = row['Nom de projet']
                
                # Basic engagement record with only required fields
                engagement = {
                    'eng_no': eng_no,
                    'eng_description': eng_description,
                    'primary_practice_id': 1  # Default to SAP practice - needed for foreign key
                }
                
                # Add client_no only if it will be available
                if 'Client No.' in row and pd.notna(row['Client No.']):
                    engagement['client_no'] = row['Client No.']
                
                engagements.append(engagement)
                
                # Basic phase record with only required fields
                phase = {
                    'eng_no': eng_no,  # First part of composite PK
                    'eng_phase': int(row['Code phase']),  # Second part of composite PK
                    'phase_description': row['Phase Projet']
                }
                
                # Add budget only if it's not NULL
                if pd.notna(row['Budget']):
                    phase['budget'] = float(row['Budget'])
                
                phases.append(phase)
            
            # Convert to DataFrames
            if engagements:
                df_engagements = pd.DataFrame(engagements)
                df_engagements = df_engagements.drop_duplicates(subset=['eng_no'])
                
                # If we already have engagements, merge with the existing ones
                if not self.dfs['engagements'].empty:
                    self.dfs['engagements'] = pd.concat([self.dfs['engagements'], df_engagements])
                    self.dfs['engagements'] = self.dfs['engagements'].drop_duplicates(subset=['eng_no'], keep='last')
                else:
                    self.dfs['engagements'] = df_engagements
                
            if phases:
                df_phases = pd.DataFrame(phases)
                # Print the first 5 rows to verify composite primary key is present
                print("\nPhases composite primary key verification (raw data):")
                print(df_phases[['eng_no', 'eng_phase']].head())
                
                # Ensure both parts of the composite key are present and not null
                df_phases = df_phases[df_phases['eng_no'].notna() & df_phases['eng_phase'].notna()]
                df_phases = df_phases.drop_duplicates(subset=['eng_no', 'eng_phase'])
                
                # If we already have phases, merge with the existing ones
                if not self.dfs['phases'].empty:
                    self.dfs['phases'] = pd.concat([self.dfs['phases'], df_phases])
                    self.dfs['phases'] = self.dfs['phases'].drop_duplicates(subset=['eng_no', 'eng_phase'], keep='last')
                else:
                    self.dfs['phases'] = df_phases
            
            print(f"Processed budget data: {len(engagements)} engagements, {len(phases)} phases")
            
        except Exception as e:
            print(f"Error processing budget data: {str(e)}")
    
    def process_staffing_data(self):
        """
        Process staffing data to populate employees, clients, and staffing tables.
        Also updates engagement client relationships.
        """
        staffing_files = [STAFFING_2024_CSV, STAFFING_2025_CSV]
        employees = []
        clients = []
        staffing_records = []
        engagement_client_map = {}
        
        for staffing_file in staffing_files:
            if not os.path.exists(staffing_file):
                print(f"Warning: Staffing file not found at {staffing_file}")
                continue
            
            try:
                print(f"Processing staffing file: {staffing_file}")
                df_staff = pd.read_csv(staffing_file, encoding='utf-8-sig')
                
                # Extract employees
                for _, row in df_staff.iterrows():
                    personnel_no = row['Personnel No.']
                    employee_name = row['Employee Name']
                    
                    # Only process valid employees (non-NULL key fields)
                    if pd.notna(personnel_no) and pd.notna(employee_name):
                        # Extract employee data
                        employees.append({
                            'personnel_no': personnel_no,
                            'employee_name': employee_name,
                            'staff_level': row['Staff Level'],
                            'is_external': False,  # Default to internal
                            'employment_basis': 40.0,  # Set 40 hours weekly basis for everyone
                            'practice_id': 1  # Default to SAP practice
                        })
                        
                        # Extract client data - only if both client_no and client_name are not NULL
                        client_no = row['Client No.']
                        client_name = row['Client Name']
                        
                        if pd.notna(client_no) and pd.notna(client_name):
                            clients.append({
                                'client_no': client_no,
                                'client_name': client_name
                            })
                            
                            # Map engagement to client only for valid clients
                            eng_no = row['Eng. No.']
                            engagement_client_map[eng_no] = client_no
                        
                        # Process staffing data - columns after 'Staff Level' are week dates
                        date_columns = [col for col in df_staff.columns[9:] if 'Unnamed' not in col]
                        
                        for date_col in date_columns:
                            hours = row[date_col]
                            if pd.notna(hours) and hours > 0:
                                # Convert string date to datetime
                                try:
                                    # Handle different date formats
                                    week_date = pd.to_datetime(date_col.replace(' 00:00:00', ''))
                                    
                                    staffing_records.append({
                                        'id': None,  # DB will generate
                                        'personnel_no': personnel_no,
                                        'eng_no': eng_no,
                                        'eng_phase': row['Eng. Phase'],
                                        'week_start_date': week_date,
                                        'planned_hours': float(hours)
                                    })
                                except:
                                    print(f"Warning: Could not parse date {date_col}")
                    else:
                        print(f"Skipping invalid employee record with NULL key fields: {row['Personnel No.']}, {row['Employee Name']}")
                
            except Exception as e:
                print(f"Error processing staffing file {staffing_file}: {str(e)}")
        
        # Convert to DataFrames and deduplicate
        if employees:
            df_employees = pd.DataFrame(employees).drop_duplicates(subset=['personnel_no'])
            
            # Extra check to remove any rows with NULL values in key fields
            df_employees = df_employees.dropna(subset=['personnel_no', 'employee_name'])
            
            self.dfs['employees'] = pd.concat([self.dfs['employees'], df_employees])
            self.dfs['employees'] = self.dfs['employees'].drop_duplicates(subset=['personnel_no'], keep='last')
            
            print(f"Processed {len(df_employees)} unique employees (after removing NULL values)")
        
        if clients:
            df_clients = pd.DataFrame(clients).drop_duplicates(subset=['client_no'])
            
            # Extra check to remove any rows with NULL values that might have slipped through
            df_clients = df_clients.dropna(subset=['client_no', 'client_name'])
            
            self.dfs['clients'] = pd.concat([self.dfs['clients'], df_clients])
            self.dfs['clients'] = self.dfs['clients'].drop_duplicates(subset=['client_no'], keep='last')
            
            print(f"Processed {len(df_clients)} unique clients (after removing NULL values)")
        
        if staffing_records:
            df_staffing = pd.DataFrame(staffing_records)
            self.dfs['staffing'] = pd.concat([self.dfs['staffing'], df_staffing])
        
        # Update engagement-client relationships
        if engagement_client_map and not self.dfs['engagements'].empty:
            for idx, row in self.dfs['engagements'].iterrows():
                eng_no = row['eng_no']
                if eng_no in engagement_client_map:
                    self.dfs['engagements'].at[idx, 'client_no'] = engagement_client_map[eng_no]
        
        print(f"Processed staffing data: {len(self.dfs['employees'])} employees, {len(self.dfs['clients'])} clients, "
              f"{len(self.dfs['staffing'])} staffing records")
    
    def process_timesheet_data(self):
        """
        Process timesheet data to populate the timesheets table.
        """
        if not os.path.exists(TIME_CSV):
            print(f"Warning: Timesheet file not found at {TIME_CSV}")
            return
        
        try:
            print(f"Processing timesheet file: {TIME_CSV}")
            
            # Try different encodings
            encodings = ['utf-8-sig', 'latin1', 'cp1252']
            df_time = None
            
            for encoding in encodings:
                try:
                    df_time = pd.read_csv(TIME_CSV, encoding=encoding)
                    break
                except:
                    continue
            
            if df_time is None or df_time.empty:
                print(f"Warning: Could not read timesheet file or file is empty")
                return
            
            # Map column names
            column_mapping = {
                'Personnel No.': 'personnel_no',
                'Work Date': 'work_date',
                'Hours': 'hours',
                'Eng. No.': 'eng_no',
                'Eng. Phase': 'eng_phase',
                'Time Entry Date': 'time_entry_date',
                'Posting Date': 'posting_date',
                'Charge-Out Rate': 'charge_out_rate',
                'Std. Price': 'std_price',
                'Adm. Surcharge': 'adm_surcharge'
            }
            
            # Check for required columns
            missing_columns = [col for col in column_mapping.keys() if col not in df_time.columns]
            if missing_columns:
                print(f"Warning: Missing columns in timesheet file: {missing_columns}")
                print(f"Available columns: {df_time.columns.tolist()}")
                return
            
            # Rename columns and set ID to None
            df_time = df_time.rename(columns=column_mapping)
            df_time['id'] = None
            
            # Convert date columns
            date_columns = ['work_date', 'time_entry_date', 'posting_date']
            for col in date_columns:
                if col in df_time.columns:
                    df_time[col] = pd.to_datetime(df_time[col], errors='coerce')
            
            # Convert numeric columns
            numeric_columns = ['hours', 'charge_out_rate', 'std_price', 'adm_surcharge']
            for col in numeric_columns:
                if col in df_time.columns:
                    df_time[col] = pd.to_numeric(df_time[col], errors='coerce')
            
            # Select only the columns defined in our schema
            schema_columns = self.schemas['timesheets']['columns']
            df_time = df_time.reindex(columns=schema_columns)
            
            # Append to the timesheets DataFrame
            self.dfs['timesheets'] = pd.concat([self.dfs['timesheets'], df_time])
            
            print(f"Processed {len(df_time)} timesheet records")
            
        except Exception as e:
            print(f"Error processing timesheet data: {str(e)}")
    
    def validate_foreign_keys(self):
        """
        Validate foreign key relationships between tables.
        """
        for table, fk_list in self.foreign_keys.items():
            if table not in self.dfs or self.dfs[table].empty:
                continue
                
            for fk in fk_list:
                if isinstance(fk[0], tuple):
                    # Composite foreign key
                    fk_cols = fk[0]
                    ref_table = fk[1]
                    ref_cols = fk[2]
                    
                    if ref_table not in self.dfs or self.dfs[ref_table].empty:
                        continue
                        
                    # Create sets of reference keys
                    ref_keys = set()
                    for _, row in self.dfs[ref_table].iterrows():
                        key_tuple = tuple(row[col] for col in ref_cols)
                        ref_keys.add(key_tuple)
                    
                    # Check each row in the table
                    valid_rows = []
                    for idx, row in self.dfs[table].iterrows():
                        fk_tuple = tuple(row[col] for col in fk_cols)
                        
                        # Allow if any FK column is NULL
                        if any(pd.isna(val) for val in fk_tuple):
                            valid_rows.append(idx)
                        # Otherwise check if the FK exists in the reference table
                        elif fk_tuple in ref_keys:
                            valid_rows.append(idx)
                    
                    # Keep only valid rows
                    self.dfs[table] = self.dfs[table].loc[valid_rows]
                
                else:
                    # Simple foreign key
                    fk_col = fk[0]
                    ref_table = fk[1]
                    ref_col = fk[2]
                    
                    if ref_table not in self.dfs or self.dfs[ref_table].empty:
                        continue
                        
                    # Get valid reference keys
                    ref_keys = set(self.dfs[ref_table][ref_col].dropna().unique())
                    
                    # Filter rows with valid foreign keys or NULL
                    self.dfs[table] = self.dfs[table][
                        self.dfs[table][fk_col].isna() | self.dfs[table][fk_col].isin(ref_keys)
                    ]
        
        # Report counts after validation
        for table, df in self.dfs.items():
            print(f"After FK validation: {table} has {len(df)} records")
    
    def save_transformed_data(self):
        """
        Save all DataFrames to CSV files in the output directory,
        explicitly ensuring composite primary keys are included.
        """
        for table, df in self.dfs.items():
            if df.empty:
                print(f"Skipping empty table: {table}")
                continue
            
            # Clean the DataFrame according to its schema
            df = self._clean_dataframe(df, table)
            
            # We've already dropped NULL columns during processing
            # so we just need to handle primary keys and set auto-generated columns
            
            # Make sure composite primary keys are present
            pk_columns = self.primary_keys.get(table, [])
            if len(pk_columns) > 1:  # Composite primary key
                print(f"\nProcessing table {table} with composite primary key {pk_columns}")
                
                # Filter to keep only rows where ALL primary key components are non-null
                valid_mask = df[pk_columns].notna().all(axis=1)
                before_filter = len(df)
                df = df[valid_mask]
                after_filter = len(df)
                
                if before_filter != after_filter:
                    print(f"Filtered out {before_filter - after_filter} rows with NULL in composite primary key columns")
                
                # Print the first 5 rows to verify composite primary key columns
                print(f"\nComposite primary key columns in {table}:")
                print(df[pk_columns].head(5))
            
            # Set auto-generated columns to NULL
            if table in self.db_generated_columns:
                for col in self.db_generated_columns[table]:
                    df[col] = None
            
            # Save to CSV with NULL representation for NaN values
            output_file = OUTPUT_DIR / f"{table}.csv"
            df.to_csv(output_file, index=False, na_rep='NULL')
            
            # Verify the saved CSV actually contains the composite primary key columns
            if len(pk_columns) > 1:
                # Read back the saved CSV to verify
                try:
                    verification_df = pd.read_csv(output_file)
                    print(f"\nVerifying composite primary key in saved CSV for {table}:")
                    print(verification_df[pk_columns].head(5))
                    
                    # Verify that all primary key columns exist
                    if all(col in verification_df.columns for col in pk_columns):
                        print(f"✓ All composite primary key columns {pk_columns} are present in the CSV")
                    else:
                        print(f"❌ ERROR: Not all composite primary key columns {pk_columns} are in the CSV!")
                        missing = [col for col in pk_columns if col not in verification_df.columns]
                        print(f"   Missing columns: {missing}")
                except Exception as e:
                    print(f"Error verifying composite primary key in saved CSV: {e}")
            
            # Report what was saved
            if len(pk_columns) > 1:
                print(f"Saved {len(df)} rows to {output_file} with composite primary key: {pk_columns}")
            else:
                print(f"Saved {len(df)} rows to {output_file}")

    def generate_placeholder_vacation_data(self):
        """
        Generate exactly 5 vacation records, with 1 vacation per employee.
        """
        if 'employees' not in self.dfs or self.dfs['employees'].empty:
            print("No employee data available to generate vacation placeholders")
            return
        
        # Define 5 vacation periods for 2025
        vacation_periods = [
            # Spring break
            ('2025-03-10', '2025-03-17'),
            # Summer vacations
            ('2025-07-01', '2025-07-15'),
            ('2025-08-03', '2025-08-17'),
            # Fall break
            ('2025-10-20', '2025-10-27'),
            # Winter holidays
            ('2025-12-22', '2025-12-31')
        ]
        
        # Get all personnel numbers from employees DataFrame
        personnel_nos = self.dfs['employees']['personnel_no'].dropna().unique().tolist()
        
        if not personnel_nos or len(personnel_nos) < 5:
            print(f"Not enough employees ({len(personnel_nos)}) to generate 5 vacations")
            return
        
        # Select exactly 5 random employees for vacations
        import random
        selected_employees = random.sample(personnel_nos, 5)
        
        # Create exactly 5 vacations, one per selected employee
        vacations = []
        for i, personnel_no in enumerate(selected_employees):
            # Assign each employee one vacation period
            start_date, end_date = vacation_periods[i]  # Use one period per employee
            
            vacations.append({
                'personnel_no': personnel_no,
                'start_date': pd.Timestamp(start_date),
                'end_date': pd.Timestamp(end_date)
            })
        
        # Create DataFrame from the 5 vacation records
        self.dfs['vacations'] = pd.DataFrame(vacations)
        
        print(f"Generated exactly 5 vacation records, 1 per employee")
        print("Vacation entries:")
        print(self.dfs['vacations'])

    def transform_all_data(self):
        """
        Execute the complete data transformation pipeline.
        """
        print(f"Starting data transformation at {datetime.now()}")
        
        # Process each data source
        self.process_budget_data()
        self.process_staffing_data()
        self.process_timesheet_data()
        # Skipping dictionary processing as requested
        print("Skipping dictionary data processing as requested")
        
        # Generate 40-hour employment basis and vacation data
        print("Setting 40-hour employment basis for all employees")
        self.generate_placeholder_vacation_data()
        
        # Update foreign keys to make sure practice IDs are properly set
        print("Setting practice IDs for all employees to SAP practice (ID 1)")
        if 'employees' in self.dfs and not self.dfs['employees'].empty:
            # Make sure ALL employees are assigned to SAP practice (ID 1)
            self.dfs['employees']['practice_id'] = 1
            
        if 'engagements' in self.dfs and not self.dfs['engagements'].empty:
            # Add some variety in engagements primary practice 
            # but ensure most are SAP (ID 1)
            self.dfs['engagements']['primary_practice_id'] = 1
            
            # For demonstration purposes, assign a few engagements to other KPMG practices
            # But keep the vast majority in SAP
            if len(self.dfs['engagements']) > 10:
                # Get some random indices, but keeping 80% in SAP
                num_to_change = min(int(len(self.dfs['engagements']) * 0.2), 7)
                if num_to_change > 0:
                    import random
                    indices = random.sample(range(len(self.dfs['engagements'])), num_to_change)
                    for i, idx in enumerate(indices):
                        # Assign to KPMG practices 2-8 (not SAP)
                        practice_id = (i % 7) + 2  # This gives practice IDs 2-8
                        self.dfs['engagements'].iloc[idx, self.dfs['engagements'].columns.get_loc('primary_practice_id')] = practice_id
            
            print(f"Assigned {num_to_change if 'num_to_change' in locals() else 0} engagements to non-SAP KPMG practices for demonstration")
        
        # Validate foreign key relationships
        self.validate_foreign_keys()
        
        # Still call save_transformed_data but it won't need to drop columns
        # since we've already handled that during processing
        self.save_transformed_data()
        
        print(f"Data transformation completed at {datetime.now()}")

def main():
    """
    Main function to run the data transformer.
    """
    try:
        transformer = DataTransformer()
        transformer.transform_all_data()
    except Exception as e:
        print(f"Error in data transformation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
